{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "from sklearn.linear_model import Ridge, RidgeCV, BayesianRidge\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, RepeatedKFold, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import xgboost as xg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "from scipy.stats import skew\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "alt.renderers.enable('default'); # if in jupyter, ; to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Exploration\n",
    "\n",
    "In this notebook we will explore several types of regression models to see which type seems to fit the data best. We will dive deeper into a few of the best performing models to see if there is any way to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Up\n",
    "\n",
    "Time columns that need to be converted to integer hour for model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parse_times = [\"MKOPEN\", \"MKCLOSE\", \"MKEMHOPEN\", \"MKEMHCLOSE\",\n",
    "               \"MKOPENYEST\", \"MKCLOSEYEST\", \"MKOPENTOM\",\n",
    "               \"MKCLOSETOM\",\"EPOPEN\", \"EPCLOSE\", \"EPEMHOPEN\",\n",
    "               \"EPEMHCLOSE\", \"EPOPENYEST\", \"EPCLOSEYEST\",\n",
    "               \"EPOPENTOM\", \"EPCLOSETOM\", \"HSOPEN\", \"HSCLOSE\",\n",
    "               \"HSEMHOPEN\", \"HSEMHCLOSE\", \"HSOPENYEST\", \"HSCLOSEYEST\",\n",
    "               \"HSOPENTOM\", \"HSCLOSETOM\", \"AKOPEN\", \"AKCLOSE\",\n",
    "               \"AKEMHOPEN\", \"AKOPENYEST\", \"AKCLOSEYEST\",\"AKEMHCLOSE\",\n",
    "               \"AKOPENTOM\", \"AKCLOSETOM\", \"MKPRDDT1\", \"MKPRDDT2\",\n",
    "               \"MKPRDNT1\", \"MKPRDNT2\", \"MKFIRET1\", \"MKFIRET2\",\n",
    "               \"EPFIRET1\", \"EPFIRET2\", \"HSPRDDT1\", \"HSFIRET1\",\n",
    "               \"HSFIRET2\", \"HSSHWNT1\", \"HSSHWNT2\", \"AKPRDDT1\",\n",
    "               \"AKPRDDT2\", \"AKSHWNT1\", \"AKSHWNT2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load posted wait time datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup():\n",
    "    with open(\"../data/processed/dtypes_parsed.json\") as json_file:\n",
    "        dtypes = json.load(json_file)\n",
    "\n",
    "    return dtypes\n",
    "\n",
    "\n",
    "def loadTrainTestPostedWaitTimes():\n",
    "    \"\"\"\n",
    "            Loads train test data for posted wait times\n",
    "\n",
    "            How to use:\n",
    "\n",
    "            import importlib.util\n",
    "\n",
    "            spec = importlib.util.spec_from_file_location(\"loadTrainTestPostedWaitTimes\", \"src/data/loadTrainTestData.py\")\n",
    "            loadTrainPosted = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(loadTrainPosted)\n",
    "\n",
    "            X_train_posted, X_test_posted, y_train_posted, y_test_posted = loadTrainPosted.loadTrainTestPostedWaitTimes()\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            rideDataDf_trainX - train data features for posted wait times\n",
    "            rideDataDf_testX - test data features for posted wait times\n",
    "            rideDataDf_trainY - train data targets for posted wait times\n",
    "            rideDataDf_testY - test data targets for posted wait times\n",
    "\n",
    "        \"\"\"\n",
    "    parse_dates = ['date', 'datetime']\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    dtypes = setup()\n",
    "\n",
    "    for year in range(2015, 2022):\n",
    "\n",
    "        rideData = pd.read_csv(f'../data/processed/All_train_postedtimes{year}.csv', dtype=dtypes,\n",
    "                               parse_dates=parse_dates, compression='gzip')\n",
    "        rideDataX = rideData.drop(columns=[\"POSTED_WAIT\"])\n",
    "        rideDataY = rideData[\"POSTED_WAIT\"]\n",
    "        X_train_list.append(rideDataX)\n",
    "        y_train_list.append(rideDataY)\n",
    "\n",
    "    rideDataDf_trainX = pd.concat(X_train_list, ignore_index=True)\n",
    "    rideDataDf_trainY = pd.concat(y_train_list, ignore_index=True)\n",
    "\n",
    "\n",
    "    X_test_list = []\n",
    "    y_test_list = []\n",
    "\n",
    "    for year in range(2015, 2022):\n",
    "        rideData = pd.read_csv(f'../data/processed/All_test_postedtimes{year}.csv', dtype=dtypes,\n",
    "                               parse_dates=parse_dates, compression='gzip')\n",
    "        rideDataX = rideData.drop(columns=[\"POSTED_WAIT\"])\n",
    "        rideDataY = rideData[\"POSTED_WAIT\"]\n",
    "\n",
    "        X_test_list.append(rideDataX)\n",
    "        y_test_list.append(rideDataY)\n",
    "\n",
    "    rideDataDf_testX = pd.concat(X_test_list, ignore_index=True)\n",
    "    rideDataDf_testY = pd.concat(y_test_list, ignore_index=True)\n",
    "\n",
    "    return rideDataDf_trainX, rideDataDf_testX, rideDataDf_trainY, rideDataDf_testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = loadTrainTestPostedWaitTimes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert key data points from date to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"MONTHOFYEAR\"] = X_train[\"date\"].dt.month.astype(\"Int8\")\n",
    "X_train[\"YEAR\"] = X_train[\"date\"].dt.year.astype(\"Int16\")\n",
    "X_train[\"DAYOFYEAR\"] = X_train[\"date\"].dt.dayofyear.astype(\"Int16\")\n",
    "X_train[\"HOUROFDAY\"] = X_train[\"datetime\"].dt.hour.astype(\"Int8\")\n",
    "\n",
    "X_test[\"MONTHOFYEAR\"] = X_test[\"date\"].dt.month.astype(\"Int8\")\n",
    "X_test[\"YEAR\"] = X_test[\"date\"].dt.year.astype(\"Int16\")\n",
    "X_test[\"DAYOFYEAR\"] = X_test[\"date\"].dt.dayofyear.astype(\"Int16\")\n",
    "X_test[\"HOUROFDAY\"] = X_test[\"datetime\"].dt.hour.astype(\"Int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by datetime before imputation (keeping y-values associated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1).sort_values(['datetime'])\n",
    "test = pd.concat([X_test, y_test], axis=1).sort_values(['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_impute = train.drop(columns=[\"POSTED_WAIT\"])\n",
    "y_train = train[\"POSTED_WAIT\"]\n",
    "\n",
    "X_test_impute = test.drop(columns=[\"POSTED_WAIT\"])\n",
    "y_test = test[\"POSTED_WAIT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many open/close times, parade times, etc. are in HH:MM format. \n",
    "\n",
    "Convert to integer hour & fill nulls with 99.\n",
    "\n",
    "This means that particulate event does not exist for that day. (e.g. Magic Kingdom doesn't have a second parade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in parse_times:\n",
    "    X_train_impute[col] =  X_train_impute[col].fillna(\"99\")\n",
    "    X_train_impute[f\"{col}_HOUR\"] = X_train_impute[col].apply(lambda x: x[:2] if x[0]!=0 else x[:1]).astype(int).astype(\"Int8\")\n",
    "    X_train_impute.drop(columns = col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in parse_times:\n",
    "    X_test_impute[col] =  X_test_impute[col].fillna(\"99\")\n",
    "    X_test_impute[f\"{col}_HOUR\"] = X_test_impute[col].apply(lambda x: x[:2] if x[0]!=0 else x[:1]).astype(int).astype(\"Int8\")\n",
    "    X_test_impute.drop(columns = col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation\n",
    "\n",
    "First impute by backfilling values. For any remaining nulls, impute the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train_impute.columns:\n",
    "    nulls = X_train_impute[col].isnull().sum()\n",
    "    \n",
    "    if nulls>0:\n",
    "        X_train_impute[col].fillna(method ='bfill', inplace=True)\n",
    "    \n",
    "        if X_train_impute[col].isnull().sum()>0:\n",
    "            X_train_impute[col].fillna(X_train_impute[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_test_impute.columns:\n",
    "    nulls = X_test_impute[col].isnull().sum()\n",
    "    if nulls>0:\n",
    "        X_test_impute[col]= X_test_impute[col].fillna(method ='bfill')\n",
    "        \n",
    "        if X_test_impute[col].isnull().sum()>0:\n",
    "            X_test_impute[col].fillna(X_test_impute[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Datetime columns - no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = X_train_impute.drop(columns=['date', 'datetime', 'Unnamed: 0'])\n",
    "X_test_encoded = X_test_impute.drop(columns=['date', 'datetime', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_impute, X_test_impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP EVENTUALLY: Use variance threshold to drop more boolean columns.\n",
    "\n",
    "These columns are more than 99.9% similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING BOOL:  []\n"
     ]
    }
   ],
   "source": [
    "X_dtype = X_train_encoded.select_dtypes(include=['bool']).reset_index(drop=True)\n",
    "\n",
    "var_thr = VarianceThreshold(threshold=0.001)  # Removing both constant and quasi-constant\n",
    "var_thr.fit(X_dtype)\n",
    "\n",
    "concol = [column for column in X_dtype.columns\n",
    "          if column not in X_dtype.columns[var_thr.get_support()]]\n",
    "\n",
    "\n",
    "del var_thr, X_dtype\n",
    "\n",
    "if \"Weather Type\" in concol:\n",
    "    concol.remove(\"Weather Type\")\n",
    "\n",
    "print(f\"DROPPING BOOL: \", concol)\n",
    "X_train_encoded.drop(concol, axis=1, inplace=True)\n",
    "X_test_encoded.drop(concol, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling & Transformation\n",
    "\n",
    "Log Transform skewed numeric columns & then apply StandardScaler to scale numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "X_dtype_train = X_train_encoded.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "num_cols = list(X_dtype_train.columns)\n",
    "\n",
    "X_dtype_test = X_test_encoded.select_dtypes(include=[np.number]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skewed_cols = []\n",
    "for col in X_dtype_train.columns:\n",
    "    skewness = abs(skew(list(X_dtype_train[col])))\n",
    "    if skewness > 1:\n",
    "        X_dtype_train[f\"log_{col}\"] = np.log(X_dtype_train[col].astype(float).apply(lambda x: x+5))\n",
    "        X_dtype_test[f\"log_{col}\"] = np.log(X_dtype_test[col].astype(float).apply(lambda x: x+5))                                         \n",
    "        skewed_cols.append(col)\n",
    "\n",
    "X_dtype_train.drop(columns=skewed_cols, inplace=True)\n",
    "X_dtype_test.drop(columns=skewed_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = scaler.fit_transform(X_dtype_train)\n",
    "X_test_norm = scaler.transform(X_dtype_test)\n",
    "\n",
    "X_train_encoded[num_cols] = X_train_norm\n",
    "X_test_encoded[num_cols] = X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_norm, X_test_norm, X_dtype_train, X_dtype_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dummy Regression - Baseline\n",
    "\n",
    "Setting baseline metrics to see improvement among other models. These models are not expected to do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train_encoded, y_train)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY REGRESSOR - MEAN\n",
      "Mean Absolute Error (MAE): 162.41068811170675\n",
      "Mean Squared Error (MSE): 84453.25944579137\n",
      "Root Mean Squared Error (RMSE): 290.6084297569349\n",
      "R-SQUARED:  -3.6194275621070915e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"DUMMY REGRESSOR - MEAN\")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_predict_dummy_mean))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_predict_dummy_mean))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_predict_dummy_mean)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, y_predict_dummy_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dummy_median = DummyRegressor(strategy = 'median').fit(X_train_encoded, y_train)\n",
    "y_predict_dummy_median = lm_dummy_median.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY REGRESSOR - MEDIAN\n",
      "Mean Absolute Error (MAE): 106.43553661159103\n",
      "Mean Squared Error (MSE): 90904.36571096092\n",
      "Root Mean Squared Error (RMSE): 301.5035086213109\n",
      "R-SQUARED:  -0.07638709517762399\n"
     ]
    }
   ],
   "source": [
    "print(\"DUMMY REGRESSOR - MEDIAN\")\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_predict_dummy_median))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_predict_dummy_median))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_predict_dummy_median)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, y_predict_dummy_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression w/ Grid Search on Alpha\n",
    "\n",
    "Testing ridge regression with differing levels of alpha. This model does not do very well compared to the dummy regressors. We will not move forward with ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linRidge = RidgeCV(alphas=[1e-1, 1, 10], scoring='neg_mean_absolute_error').fit(X_train_encoded, y_train)\n",
    "predLinRidge = linRidge.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE REGRESSION GRID SEARCH: \n",
      "Mean Absolute Error (MAE): 141.39446762707306\n",
      "Mean Squared Error (MSE): 69836.10725051745\n",
      "Root Mean Squared Error (RMSE): 264.2652214168892\n",
      "R-SQUARED:  0.17307948816330976\n"
     ]
    }
   ],
   "source": [
    "print(\"RIDGE REGRESSION GRID SEARCH: \")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, predLinRidge))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, predLinRidge))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, predLinRidge)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, predLinRidge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost Regressor\n",
    "\n",
    "```n_estimators = 100```\n",
    "\n",
    "XGBoost regressor does significantly better than the dummy regressors, however it's not doing as well as we'd hope. We will put this one aside for now and continue exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb_r = xg.XGBRegressor(objective ='reg:squarederror',\n",
    "                  n_estimators = 100, seed = 123)\n",
    "# Fitting the model\n",
    "xgb_r.fit(X_train_encoded, y_train)\n",
    "# Predict the model\n",
    "pred = xgb_r.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost n_estimators = 100\n",
      "Mean Absolute Error (MAE): 81.98376024978742\n",
      "Mean Squared Error (MSE): 38018.90601036953\n",
      "Root Mean Squared Error (RMSE): 194.9843737594619\n",
      "R-SQUARED:  0.5498229432406851\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost n_estimators = 100\")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian Ridge Regressor\n",
    "\n",
    "Bayesian Ridge regressor has similar results to traditional Ridge which does not perform well. We will not move forward with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BayReg = BayesianRidge()\n",
    "BayReg.fit(X_train_encoded, y_train)\n",
    "pred = BayReg.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Ridge Regression: \n",
      "Mean Absolute Error (MAE): 141.4040042726355\n",
      "Mean Squared Error (MSE): 69840.6091746251\n",
      "Root Mean Squared Error (RMSE): 264.2737390938137\n",
      "R-SQUARED:  0.1730261814494347\n"
     ]
    }
   ],
   "source": [
    "print(\"Bayesian Ridge Regression: \")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------------------\n",
    "\n",
    "# Tree-Based Model Deep Dive\n",
    "\n",
    "Our analysis showed us that tree based models do fairly well on this data. We will further explore these models to determine if we can maximize the performance and minimize the complexity of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "We resorted to doing hyperparameter tuning manually because GCP threw a TerminatedWorkerError during GridSearchCV and RandomizedSearchCV efforts and running the same thing locally threw memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING 10, 10\n",
      "STARTING 10, 50\n",
      "STARTING 10, 100\n",
      "STARTING 50, 10\n",
      "STARTING 50, 50\n"
     ]
    }
   ],
   "source": [
    "important_features = {}\n",
    "rfResults = {}\n",
    "feature_dict = defaultdict(list)\n",
    "rf_feature_importance = defaultdict(dict)\n",
    "for n_est in (10, 50, 100):\n",
    "    for max_depth in [10, 50, 100]:\n",
    "        print(f\"STARTING {n_est}, {max_depth}\")\n",
    "        rfc = RandomForestRegressor(n_estimators=n_est, max_depth=max_depth, random_state=0, n_jobs=-1)\n",
    "        rfc.fit(X_train_encoded, y_train)\n",
    "        \n",
    "        pred = rfc.predict(X_test_encoded)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(y_test, pred)\n",
    "        mse = metrics.mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "        r2 = metrics.r2_score(y_test, pred)\n",
    "        \n",
    "        rfResults[f\"{n_est}_{max_depth}\"] = {\"mae\": mae, \"mse\": mse, \"rmse\":rmse, \"r2\":r2}\n",
    "        \n",
    "        importances = rfc.feature_importances_\n",
    "        important_feat = np.argsort(importances)[-50:]\n",
    "        \n",
    "        selected_feat_names= list(X_train.columns[important_feat])\n",
    "        \n",
    "        for idx, x in enumerate(important_feat):\n",
    "            feature_dict[selected_feat_names[idx]].append(importances[x])\n",
    "            \n",
    "            try:\n",
    "                rf_feature_importance[selected_feat_names[idx]][\"count\"] += 1\n",
    "                rf_feature_importance[selected_feat_names[idx]][\"importances\"].append(importances[x])\n",
    "            except KeyError:\n",
    "                rf_feature_importance[selected_feat_names[idx]][\"count\"] = 1\n",
    "                rf_feature_importance[selected_feat_names[idx]][\"importances\"] = [importances[x]]\n",
    "            \n",
    "    \n",
    "print(rfResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RandomForest Grid Search Results\n",
    "\n",
    "It appears that the sweet spot for metrics optimization vs. model complexity is **10 estimators and max depth of 50 for each tree**. Additional trees beyond 10 does not add value to the model and there is a sigificant reduction in error and increase in r-squared after increasing max-depth to 50. However, more than 50 max_depth for each tree does not seem to increase value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-3f8c824c4d274e68afb7900af8342af4\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-3f8c824c4d274e68afb7900af8342af4\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-3f8c824c4d274e68afb7900af8342af4\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-987fd190ab392b8072910d4d8bf75906\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"variable\", \"scale\": {\"range\": [\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"]}, \"type\": \"nominal\"}, \"column\": {\"field\": \"variable\", \"type\": \"nominal\"}, \"row\": {\"field\": \"n_estimators\", \"type\": \"nominal\"}, \"x\": {\"field\": \"max_depth\", \"type\": \"nominal\"}, \"y\": {\"field\": \"value\", \"type\": \"quantitative\"}}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": {\"text\": [\"Performance Metrics by n_estimators & max_depth\"], \"subtitle\": [\"Random Forest Model\"]}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-987fd190ab392b8072910d4d8bf75906\": [{\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 82.3620180133828}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.457268737194994}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.42807329716989}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 82.13680600195903}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.29108059016234}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.281353116397874}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 81.96562853215062}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.242351815370895}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.234521452666414}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 42287.0163817075}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22579.505254077438}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22584.17400055107}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 42014.967179508305}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22208.973535539466}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22215.283708694125}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 41904.37406963158}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22165.243728735346}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22170.882168002983}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 205.63807133336837}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 150.2647838120344}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 150.28031807442738}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 204.97552824546716}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 149.02675442865777}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 149.04792420122504}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 204.7055789900011}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 148.87996416151955}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 148.8988991497351}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.499284788240413}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7326389345451009}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.732583652508693}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.5025060884300173}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7370263537528471}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.7369516357907461}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.5038156074542782}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7375441528634886}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.7374773888173709}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfGridSearchResults = pd.DataFrame.from_dict(rfResults).T.reset_index()\n",
    "\n",
    "rfGridSearchResults[['n_estimators', 'max_depth']] = rfGridSearchResults['index'].str.split(\"_\", expand=True).astype(int)\n",
    "rfGridSearchResults = rfGridSearchResults.melt(id_vars=[\"index\", \"n_estimators\", \"max_depth\"])\n",
    "\n",
    "rfMetricsChart = alt.Chart(rfGridSearchResults).mark_bar().encode(\n",
    "    x='max_depth:N',\n",
    "    y='value:Q',\n",
    "    color=alt.Color('variable:N', scale = alt.Scale(range=[\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"])),\n",
    "    row='n_estimators:N',\n",
    "    column='variable:N',\n",
    ").resolve_scale(\n",
    "  y='independent'\n",
    ").properties(\n",
    "    title={\n",
    "    \"text\": [\"Performance Metrics by n_estimators & max_depth\"],\n",
    "      \"subtitle\": [\"Random Forest Model\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "save(rfMetricsChart, \"../reports/figures/GridSearchRandomForest.html\")\n",
    "\n",
    "rfMetricsChart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Features for Random Forest\n",
    "\n",
    "Top 50 Features sorted on how many of the 9 Random Forest models considered it important and mean importance among those models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20=sorted(rf_feature_importance, key=lambda x: (-rf_feature_importance[x]['count'], -rf_feature_importance[x][\"mean_importance\"]))[:20]\n",
    "rf_feature_importance[top20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EPOPENYEST',\n",
       " 'Age_interest_adults',\n",
       " 'Age_interest_teens',\n",
       " 'Height_req_inches',\n",
       " 'WDWSEASON_none',\n",
       " 'HSHOURSYEST',\n",
       " 'EPHOURSYEST',\n",
       " 'MKOPENTOM',\n",
       " 'MKFIREN_fantasy in the sky fireworks',\n",
       " 'AKOPEN',\n",
       " 'EPEMHOPEN',\n",
       " 'WDWSEASON_columbus day',\n",
       " 'MKCLOSEYEST',\n",
       " 'MKHOURSYEST',\n",
       " 'MKCLOSETOM',\n",
       " 'WDWSEASON_memorial day',\n",
       " 'WDWSEASON_september low',\n",
       " 'DAYOFYEAR',\n",
       " 'MKOPENYEST',\n",
       " 'HOLIDAYN_lab',\n",
       " 'INSESSION_PLANES',\n",
       " 'MKFIREN_happily ever after',\n",
       " 'Age_of_ride_days',\n",
       " 'MKeventN_mvmcp',\n",
       " 'HSOPENTOM',\n",
       " 'Age_of_ride_years',\n",
       " 'MKeventN_mnsshp',\n",
       " 'WDWSEASON_thanksgiving',\n",
       " 'inSession_NY_NJ',\n",
       " 'WDWeventN_wdwdd|wdwhol',\n",
       " 'EPCLOSEYEST',\n",
       " 'Wind Speed Quality_p',\n",
       " 'Ride_type_slow',\n",
       " 'MKPRDNN_main street electrical parade',\n",
       " 'MKEMHCLOSE',\n",
       " 'Ride_name_dumbo the flying elephant',\n",
       " 'HSEMHEYEST',\n",
       " 'inSession',\n",
       " 'MKEMHMORN',\n",
       " 'MKPRDNN_mickey\\'s \"boo-to-you\" halloween parade',\n",
       " 'MKPRDNN_none',\n",
       " \"MKFIREN_disney's celebrate america! - a fourth of july concert in the sky\",\n",
       " 'HSHOURSTOM',\n",
       " 'WEEKOFYEAR',\n",
       " \"MKFIREN_disney's not so spooky spectacular\",\n",
       " 'DAYOFWEEK',\n",
       " 'inSession_NY_NJ_PA',\n",
       " 'MONTHOFYEAR',\n",
       " 'Age_interest_tweens',\n",
       " 'inSession_New_England']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in rf_feature_importance:\n",
    "    rf_feature_importance[x][\"mean_importance\"] = np.mean(rf_feature_importance[x][\"importances\"])\n",
    "    \n",
    "sorted(rf_feature_importance, key=lambda x: (-rf_feature_importance[x]['count'], -rf_feature_importance[x][\"mean_importance\"]))[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Regressor\n",
    "\n",
    "Extra Trees regressor is supposed to help with overfitting in RandomForest models. We tried this model to see if we could improve the performance on the test set, however, we got very similar results to Random Forest so have decided not to go with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "important_features = {}\n",
    "etResults = {}\n",
    "feature_dict = defaultdict(list)\n",
    "et_feature_importance = defaultdict(dict)\n",
    "for n_est in (10, 50, 100):\n",
    "    for max_depth in [10, 50, 100]:\n",
    "        print(f\"STARTING {n_est}, {max_depth}\")\n",
    "        etr = ExtraTreesRegressor(n_estimators=n_est, max_depth=max_depth, random_state=0, n_jobs=-1)\n",
    "        etr.fit(X_train_encoded, y_train)\n",
    "        \n",
    "        pred = etr.predict(X_test_encoded)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(y_test, pred)\n",
    "        mse = metrics.mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "        r2 = metrics.r2_score(y_test, pred)\n",
    "        \n",
    "        etResults[f\"{n_est}_{max_depth}\"] = {\"mae\": mae, \"mse\": mse, \"rmse\":rmse, \"r2\":r2}\n",
    "        \n",
    "        importances = etr.feature_importances_\n",
    "        important_feat = np.argsort(importances)[-50:]\n",
    "        \n",
    "        selected_feat_names= list(X_train.columns[important_feat])\n",
    "        \n",
    "        for idx, x in enumerate(important_feat):\n",
    "            feature_dict[selected_feat_names[idx]].append(importances[x])\n",
    "            \n",
    "            try:\n",
    "                et_feature_importance[selected_feat_names[idx]][\"count\"] += 1\n",
    "                et_feature_importance[selected_feat_names[idx]][\"importances\"].append(importances[x])\n",
    "            except KeyError:\n",
    "                et_feature_importance[selected_feat_names[idx]][\"count\"] = 1\n",
    "                et_feature_importance[selected_feat_names[idx]][\"importances\"] = [importances[x]]\n",
    "            \n",
    "    \n",
    "print(etResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Trees Grid Search Results\n",
    "\n",
    "It appears that the sweet spot for metrics optimization vs. model complexity is 10 estimators and max of 50 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-fa863f65df3a4df39660672662e669bd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-fa863f65df3a4df39660672662e669bd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-fa863f65df3a4df39660672662e669bd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-8be47301a50598689bdd2e08d7940838\"}, \"mark\": \"bar\", \"encoding\": {\"color\": {\"field\": \"variable\", \"scale\": {\"range\": [\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"]}, \"type\": \"nominal\"}, \"column\": {\"field\": \"variable\", \"type\": \"nominal\"}, \"row\": {\"field\": \"n_estimators\", \"type\": \"nominal\"}, \"x\": {\"field\": \"max_depth\", \"type\": \"nominal\"}, \"y\": {\"field\": \"value\", \"type\": \"quantitative\"}}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": {\"text\": [\"Performance Metrics by n_estimators & max_depth\"], \"subtitle\": [\"Extra Trees Model\"]}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-8be47301a50598689bdd2e08d7940838\": [{\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 92.95140587529919}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.13122904753853}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.139040173664895}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 92.58374706149999}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.12996160162398}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.130332094998316}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"mae\", \"value\": 92.20082295970086}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"mae\", \"value\": 40.13141886553458}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"mae\", \"value\": 40.12993800060594}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 44700.12928111365}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22941.65829571273}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22952.065080439406}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 44305.13164006113}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22925.948211568233}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22929.292872811064}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"mse\", \"value\": 44179.23326329713}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"mse\", \"value\": 22925.110779058265}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"mse\", \"value\": 22927.271924540495}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 211.4240508577812}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 151.46503984653597}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 151.49938970319124}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 210.48784202433436}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 151.41317053535414}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 151.42421494863714}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"rmse\", \"value\": 210.18856596707903}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"rmse\", \"value\": 151.410405121505}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"rmse\", \"value\": 151.41754166720742}, {\"index\": \"10_10\", \"n_estimators\": 10, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.47071142365211305}, {\"index\": \"10_50\", \"n_estimators\": 10, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7283507261906748}, {\"index\": \"10_100\", \"n_estimators\": 10, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.7282275007691619}, {\"index\": \"50_10\", \"n_estimators\": 50, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.4753885407534678}, {\"index\": \"50_50\", \"n_estimators\": 50, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7285367473097379}, {\"index\": \"50_100\", \"n_estimators\": 50, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.728497143599054}, {\"index\": \"100_10\", \"n_estimators\": 100, \"max_depth\": 10, \"variable\": \"r2\", \"value\": 0.4768792875068577}, {\"index\": \"100_50\", \"n_estimators\": 100, \"max_depth\": 50, \"variable\": \"r2\", \"value\": 0.7285466632421551}, {\"index\": \"100_100\", \"n_estimators\": 100, \"max_depth\": 100, \"variable\": \"r2\", \"value\": 0.7285210733918803}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etGridSearchResults = pd.DataFrame.from_dict(etResults).T.reset_index()\n",
    "\n",
    "etGridSearchResults[['n_estimators', 'max_depth']] = etGridSearchResults['index'].str.split(\"_\", expand=True).astype(int)\n",
    "etGridSearchResults = etGridSearchResults.melt(id_vars=[\"index\", \"n_estimators\", \"max_depth\"])\n",
    "\n",
    "etMetricsChart = alt.Chart(etGridSearchResults).mark_bar().encode(\n",
    "    x='max_depth:N',\n",
    "    y='value:Q',\n",
    "    color=alt.Color('variable:N', scale = alt.Scale(range=[\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"])),\n",
    "    row='n_estimators:N',\n",
    "    column='variable:N',\n",
    ").resolve_scale(\n",
    "  y='independent'\n",
    ").properties(\n",
    "    title={\n",
    "    \"text\": [\"Performance Metrics by n_estimators & max_depth\"],\n",
    "      \"subtitle\": [\"Extra Trees Model\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "save(etMetricsChart, '../reports/figures/GridSearchExtraTrees.html')\n",
    "etMetricsChart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Features for Extra Trees\n",
    "\n",
    "Top 50 Features sorted on how many of the 9 Extra trees models considered it important and mean importance among those models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WDW_TICKET_SEASON_none',\n",
       " 'Cloud Quality Code_passed all quality control checks',\n",
       " 'CapacityLostWGT_MK',\n",
       " 'CapacityLost_HS',\n",
       " 'HSCLOSE',\n",
       " 'HSOPEN',\n",
       " 'EPHOURSTOM',\n",
       " 'EPCLOSETOM',\n",
       " 'EPOPENTOM',\n",
       " 'EPHOURSYEST',\n",
       " 'EPCLOSEYEST',\n",
       " 'EPOPENYEST',\n",
       " 'EPEMHCLOSE',\n",
       " 'HSHOURS',\n",
       " 'EPEMHOPEN',\n",
       " 'EPCLOSE',\n",
       " 'EPOPEN',\n",
       " 'MKHOURSTOM',\n",
       " 'MKCLOSETOM',\n",
       " 'MKOPENTOM',\n",
       " 'MKHOURSYEST',\n",
       " 'MKCLOSEYEST',\n",
       " 'MKOPENYEST',\n",
       " 'MKEMHCLOSE',\n",
       " 'EPHOURS',\n",
       " 'CapacityLost_AK',\n",
       " 'HSEMHOPEN',\n",
       " 'HSOPENYEST',\n",
       " 'CapacityLost_EP',\n",
       " 'CapacityLost_MK',\n",
       " 'WEATHER_WDWLOW',\n",
       " 'WEATHER_WDWHIGH',\n",
       " 'AKHOURSTOM',\n",
       " 'AKCLOSETOM',\n",
       " 'AKOPENTOM',\n",
       " 'AKHOURSYEST',\n",
       " 'AKCLOSEYEST',\n",
       " 'HSEMHCLOSE',\n",
       " 'AKOPENYEST',\n",
       " 'AKEMHOPEN',\n",
       " 'AKHOURS',\n",
       " 'AKCLOSE',\n",
       " 'AKOPEN',\n",
       " 'HSHOURSTOM',\n",
       " 'HSCLOSETOM',\n",
       " 'HSOPENTOM',\n",
       " 'HSHOURSYEST',\n",
       " 'HSCLOSEYEST',\n",
       " 'AKEMHCLOSE',\n",
       " 'Cloud Quality Code_passed all quality control checks, data originate from an ncei data source']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in et_feature_importance:\n",
    "    et_feature_importance[x][\"mean_importance\"] = np.mean(et_feature_importance[x][\"importances\"])\n",
    "    \n",
    "sorted(et_feature_importance, key=lambda x: (-et_feature_importance[x]['count'], -et_feature_importance[x][\"mean_importance\"]))[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING Max Depth 10\n",
      "STARTING Max Depth 50\n",
      "STARTING Max Depth 100\n",
      "STARTING Max Depth None\n",
      "{'MaxDepth10': {'mae': 82.5697016324016, 'mse': 42796.51522047903, 'rmse': 206.87318632553382, 'r2': 0.4932518769410217}, 'MaxDepth50': {'mae': 40.19006682345901, 'mse': 23210.514305829132, 'rmse': 152.3499731074119, 'r2': 0.7251672361845908}, 'MaxDepth100': {'mae': 40.165132561672415, 'mse': 23198.939656900307, 'rmse': 152.31198133075515, 'r2': 0.725304290138389}, 'MaxDepthNone': {'mae': 40.165132561672415, 'mse': 23198.939656900307, 'rmse': 152.31198133075515, 'r2': 0.725304290138389}}\n"
     ]
    }
   ],
   "source": [
    "important_features = {}\n",
    "dtResults = {}\n",
    "feature_dict = defaultdict(list)\n",
    "dt_feature_importance = defaultdict(dict)\n",
    "for max_depth in [10, 50, 100, None]:\n",
    "    print(f\"STARTING Max Depth {max_depth}\")\n",
    "    dt = DecisionTreeRegressor(max_depth=max_depth, random_state=0)\n",
    "    dt.fit(X_train_encoded, y_train)\n",
    "\n",
    "    pred = dt.predict(X_test_encoded)\n",
    "\n",
    "    mae = metrics.mean_absolute_error(y_test, pred)\n",
    "    mse = metrics.mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "    r2 = metrics.r2_score(y_test, pred)\n",
    "\n",
    "    dtResults[f\"MaxDepth{max_depth}\"] = {\"mae\": mae, \"mse\": mse, \"rmse\":rmse, \"r2\":r2}\n",
    "\n",
    "    importances = dt.feature_importances_\n",
    "    important_feat = np.argsort(importances)[-50:]\n",
    "\n",
    "    selected_feat_names= list(X_train.columns[important_feat])\n",
    "\n",
    "    for idx, x in enumerate(important_feat):\n",
    "        feature_dict[selected_feat_names[idx]].append(importances[x])\n",
    "\n",
    "        try:\n",
    "            dt_feature_importance[selected_feat_names[idx]][\"count\"] += 1\n",
    "            dt_feature_importance[selected_feat_names[idx]][\"importances\"].append(importances[x])\n",
    "        except KeyError:\n",
    "            dt_feature_importance[selected_feat_names[idx]][\"count\"] = 1\n",
    "            dt_feature_importance[selected_feat_names[idx]][\"importances\"] = [importances[x]]\n",
    "\n",
    "\n",
    "print(dtResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtGridSearchResults = pd.DataFrame.from_dict(dtResults).T.reset_index()\n",
    "\n",
    "dtGridSearchResults['max_depth'] = dtGridSearchResults['index'].astype(int)\n",
    "dtGridSearchResults.drop('index', inplace=True)\n",
    "dtGridSearchResults = dtGridSearchResults.melt(id_vars=[\"max_depth\"])\n",
    "print(dtGridSearchResults)\n",
    "dtMetricsChart = alt.Chart(dtGridSearchResults).mark_bar().encode(\n",
    "    x='max_depth:N',\n",
    "    y='value:Q',\n",
    "    color=alt.Color('variable:N', scale = alt.Scale(range=[\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"])),\n",
    "    row='max_depth:N',\n",
    "    column='variable:N',\n",
    ").resolve_scale(\n",
    "  y='independent'\n",
    ").properties(\n",
    "    title={\n",
    "    \"text\": [\"Performance Metrics by max_depth\"],\n",
    "      \"subtitle\": [\"Decision Tree Model\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "save(dtMetricsChart, '../reports/figures/GridSearchDecisionTree.html')\n",
    "dtMetricsChart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
