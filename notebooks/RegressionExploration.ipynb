{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import importlib.util\n",
    "from sklearn.linear_model import Ridge, RidgeCV, BayesianRidge\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, RepeatedKFold, GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import xgboost as xg\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import altair as alt\n",
    "from scipy.stats import skew\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Time columns that need to be converted to integer hour for model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parse_times = [\"MKOPEN\", \"MKCLOSE\", \"MKEMHOPEN\", \"MKEMHCLOSE\",\n",
    "               \"MKOPENYEST\", \"MKCLOSEYEST\", \"MKOPENTOM\",\n",
    "               \"MKCLOSETOM\",\"EPOPEN\", \"EPCLOSE\", \"EPEMHOPEN\",\n",
    "               \"EPEMHCLOSE\", \"EPOPENYEST\", \"EPCLOSEYEST\",\n",
    "               \"EPOPENTOM\", \"EPCLOSETOM\", \"HSOPEN\", \"HSCLOSE\",\n",
    "               \"HSEMHOPEN\", \"HSEMHCLOSE\", \"HSOPENYEST\", \"HSCLOSEYEST\",\n",
    "               \"HSOPENTOM\", \"HSCLOSETOM\", \"AKOPEN\", \"AKCLOSE\",\n",
    "               \"AKEMHOPEN\", \"AKOPENYEST\", \"AKCLOSEYEST\",\"AKEMHCLOSE\",\n",
    "               \"AKOPENTOM\", \"AKCLOSETOM\", \"MKPRDDT1\", \"MKPRDDT2\",\n",
    "               \"MKPRDNT1\", \"MKPRDNT2\", \"MKFIRET1\", \"MKFIRET2\",\n",
    "               \"EPFIRET1\", \"EPFIRET2\", \"HSPRDDT1\", \"HSFIRET1\",\n",
    "               \"HSFIRET2\", \"HSSHWNT1\", \"HSSHWNT2\", \"AKPRDDT1\",\n",
    "               \"AKPRDDT2\", \"AKSHWNT1\", \"AKSHWNT2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load posted wait time datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the module that needs to be\n",
    "# imported relative to the path of the\n",
    "# module\n",
    "spec = importlib.util.spec_from_file_location(\"loadTrainTestPostedWaitTimes\", \"../src/data/loadTrainTestData.py\")\n",
    "\n",
    "# creates a new module based on spec\n",
    "loadTrainPosted = importlib.util.module_from_spec(spec)\n",
    "\n",
    "# executes the module in its own namespace\n",
    "# when a module is imported or reloaded.\n",
    "spec.loader.exec_module(loadTrainPosted)\n",
    "\n",
    "X_train, X_test, y_train, y_test = loadTrainPosted.loadTrainTestPostedWaitTimes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert key data points from date to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"MONTHOFYEAR\"] = X_train[\"date\"].dt.month.astype(\"Int8\")\n",
    "X_train[\"YEAR\"] = X_train[\"date\"].dt.year.astype(\"Int16\")\n",
    "X_train[\"DAYOFYEAR\"] = X_train[\"date\"].dt.dayofyear.astype(\"Int16\")\n",
    "X_train[\"HOUROFDAY\"] = X_train[\"datetime\"].dt.hour.astype(\"Int8\")\n",
    "\n",
    "X_test[\"MONTHOFYEAR\"] = X_test[\"date\"].dt.month.astype(\"Int8\")\n",
    "X_test[\"YEAR\"] = X_test[\"date\"].dt.year.astype(\"Int16\")\n",
    "X_test[\"DAYOFYEAR\"] = X_test[\"date\"].dt.dayofyear.astype(\"Int16\")\n",
    "X_test[\"HOUROFDAY\"] = X_test[\"datetime\"].dt.hour.astype(\"Int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by datetime before imputation (keeping y-values associated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1).sort_values(['datetime'])\n",
    "test = pd.concat([X_test, y_test], axis=1).sort_values(['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_impute = train.drop(columns=[\"POSTED_WAIT\"])\n",
    "y_train = train[\"POSTED_WAIT\"]\n",
    "\n",
    "X_test_impute = test.drop(columns=[\"POSTED_WAIT\"])\n",
    "y_test = test[\"POSTED_WAIT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many open/close times, parade times, etc. are in HH:MM format. \n",
    "\n",
    "Convert to integer hour & fill nulls with 99.\n",
    "\n",
    "This means that particulate event does not exist for that day. (e.g. Magic Kingdom doesn't have a second parade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in parse_times:\n",
    "    X_train_impute[col] =  X_train_impute[col].fillna(\"99\")\n",
    "    X_train_impute[f\"{col}_HOUR\"] = X_train_impute[col].apply(lambda x: x[:2] if x[0]!=0 else x[:1]).astype(int).astype(\"Int8\")\n",
    "    X_train_impute.drop(columns = col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in parse_times:\n",
    "    X_test_impute[col] =  X_test_impute[col].fillna(\"99\")\n",
    "    X_test_impute[f\"{col}_HOUR\"] = X_test_impute[col].apply(lambda x: x[:2] if x[0]!=0 else x[:1]).astype(int).astype(\"Int8\")\n",
    "    X_test_impute.drop(columns = col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation\n",
    "\n",
    "First impute by backfilling values. For any remaining nulls, impute the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WDWMAXTEMP\n",
      "WDWMINTEMP\n",
      "WDWMEANTEMP\n",
      "inSession\n",
      "inSession_Enrollment\n",
      "inSession_wdw\n",
      "inSession_dlr\n",
      "inSession_sqrt_WDW\n",
      "inSession_sqrt_DLR\n",
      "inSession_California\n",
      "inSession_DC\n",
      "inSession_Central_FL\n",
      "inSession_Drive1_FL\n",
      "inSession_Drive2_FL\n",
      "inSession_Drive_CA\n",
      "inSession_Florida\n",
      "inSession_Mardi_Gras\n",
      "inSession_Midwest\n",
      "inSession_NY_NJ\n",
      "inSession_NY_NJ_PA\n",
      "inSession_New_England\n",
      "inSession_New_Jersey\n",
      "inSession_Nothwest\n",
      "INSESSION_PLANES\n",
      "inSession_SoCal\n",
      "inSession_Southwest\n"
     ]
    }
   ],
   "source": [
    "for col in X_train_impute.columns:\n",
    "    nulls = X_train_impute[col].isnull().sum()\n",
    "    \n",
    "    if nulls>0:\n",
    "        print(col)\n",
    "        X_train_impute[col].fillna(method ='bfill', inplace=True)\n",
    "    \n",
    "        if X_train_impute[col].isnull().sum()>0:\n",
    "            X_train_impute[col].fillna(X_train_impute[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WDWMAXTEMP\n",
      "WDWMINTEMP\n",
      "WDWMEANTEMP\n",
      "inSession\n",
      "inSession_Enrollment\n",
      "inSession_wdw\n",
      "inSession_dlr\n",
      "inSession_sqrt_WDW\n",
      "inSession_sqrt_DLR\n",
      "inSession_California\n",
      "inSession_DC\n",
      "inSession_Central_FL\n",
      "inSession_Drive1_FL\n",
      "inSession_Drive2_FL\n",
      "inSession_Drive_CA\n",
      "inSession_Florida\n",
      "inSession_Mardi_Gras\n",
      "inSession_Midwest\n",
      "inSession_NY_NJ\n",
      "inSession_NY_NJ_PA\n",
      "inSession_New_England\n",
      "inSession_New_Jersey\n",
      "inSession_Nothwest\n",
      "INSESSION_PLANES\n",
      "inSession_SoCal\n",
      "inSession_Southwest\n"
     ]
    }
   ],
   "source": [
    "for col in X_test_impute.columns:\n",
    "    nulls = X_test_impute[col].isnull().sum()\n",
    "    \n",
    "    if nulls>0:\n",
    "        print(col)\n",
    "        X_test_impute[col]= X_test_impute[col].fillna(method ='bfill')\n",
    "        \n",
    "        if X_test_impute[col].isnull().sum()>0:\n",
    "            X_test_impute[col].fillna(X_test_impute[col].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Datetime columns - no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = X_train_impute.drop(columns=['date', 'datetime', 'Unnamed: 0'])\n",
    "X_test_encoded = X_test_impute.drop(columns=['date', 'datetime', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_impute, X_test_impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use variance threshold to drop more boolean columns.\n",
    "\n",
    "These columns are more than 99.9% similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROPPING BOOL:  ['HOLIDAYN_ash|val', 'HOLIDAYN_chv|pas', 'HOLIDAYN_cmd|han', 'HOLIDAYN_col|suk', 'HOLIDAYN_hal|nvd', 'HOLIDAYN_njc|vet', 'MKeventN_dah|emm', 'HSeventN_wdwsotf', \"HSFIREN_disney's hollywood studios special july 4th fireworks presentation\", \"HSFIREN_new year's eve fireworks\", 'Wind Speed Quality_a', 'Wind Speed Quality_p', 'Wind Speed Quality_passed gross limits check if element is present', 'Cloud Quality Code_erroneous, data originate from an ncei data source', 'Cloud Determination Code_statistically derived', 'Visibiliy Quality Code_p', 'Visibility Variability Code_variable', 'Temperature Quality Code_suspect, data originate from an ncei data source']\n"
     ]
    }
   ],
   "source": [
    "X_dtype = X_train_encoded.select_dtypes(include=['bool']).reset_index(drop=True)\n",
    "\n",
    "var_thr = VarianceThreshold(threshold=0.001)  # Removing both constant and quasi-constant\n",
    "var_thr.fit(X_dtype)\n",
    "\n",
    "concol = [column for column in X_dtype.columns\n",
    "          if column not in X_dtype.columns[var_thr.get_support()]]\n",
    "\n",
    "\n",
    "del var_thr, X_dtype\n",
    "\n",
    "if \"Weather Type\" in concol:\n",
    "    concol.remove(\"Weather Type\")\n",
    "\n",
    "print(f\"DROPPING BOOL: \", concol)\n",
    "X_train_encoded.drop(concol, axis=1, inplace=True)\n",
    "X_test_encoded.drop(concol, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling & Transformation\n",
    "\n",
    "Log Transform skewed numeric columns & then apply StandardScaler to scale numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "X_dtype_train = X_train_encoded.select_dtypes(include=[np.number]).reset_index(drop=True)\n",
    "num_cols = list(X_dtype_train.columns)\n",
    "\n",
    "X_dtype_test = X_test_encoded.select_dtypes(include=[np.number]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ride_duration_min\n",
      "HOLIDAYPX\n",
      "HOLIDAYM\n",
      "WDWMAXTEMP\n",
      "inSession_SoCal\n",
      "CapacityLost_HS\n",
      "CapacityLost_AK\n",
      "CapacityLostWGT_HS\n",
      "CapacityLostWGT_AK\n",
      "MKPRDDAY\n",
      "MKFIREWK\n",
      "EPFIREWK\n",
      "new_case\n",
      "Wind Angle\n",
      "Wind Speed\n",
      "Cloud Height\n",
      "Visibility Distance (M)\n",
      "MKOPEN_HOUR\n",
      "MKCLOSE_HOUR\n",
      "MKOPENYEST_HOUR\n",
      "MKCLOSEYEST_HOUR\n",
      "MKOPENTOM_HOUR\n",
      "MKCLOSETOM_HOUR\n",
      "EPOPEN_HOUR\n",
      "EPCLOSE_HOUR\n",
      "EPEMHOPEN_HOUR\n",
      "EPEMHCLOSE_HOUR\n",
      "EPOPENYEST_HOUR\n",
      "EPCLOSEYEST_HOUR\n",
      "EPOPENTOM_HOUR\n",
      "EPCLOSETOM_HOUR\n",
      "HSOPEN_HOUR\n",
      "HSCLOSE_HOUR\n",
      "HSEMHOPEN_HOUR\n",
      "HSEMHCLOSE_HOUR\n",
      "HSOPENYEST_HOUR\n",
      "HSCLOSEYEST_HOUR\n",
      "HSOPENTOM_HOUR\n",
      "HSCLOSETOM_HOUR\n",
      "AKOPEN_HOUR\n",
      "AKCLOSE_HOUR\n",
      "AKOPENYEST_HOUR\n",
      "AKCLOSEYEST_HOUR\n",
      "AKOPENTOM_HOUR\n",
      "AKCLOSETOM_HOUR\n",
      "MKPRDDT1_HOUR\n",
      "MKPRDDT2_HOUR\n",
      "MKFIRET1_HOUR\n",
      "MKFIRET2_HOUR\n",
      "EPFIRET1_HOUR\n",
      "EPFIRET2_HOUR\n",
      "HSFIRET1_HOUR\n",
      "HSFIRET2_HOUR\n",
      "HSSHWNT1_HOUR\n"
     ]
    }
   ],
   "source": [
    "skewed_cols = []\n",
    "for col in X_dtype_train.columns:\n",
    "    skewness = abs(skew(list(X_dtype_train[col])))\n",
    "    if skewness > 1:\n",
    "        print(col)\n",
    "        X_dtype_train[f\"log_{col}\"] = np.log(X_dtype_train[col].astype(float).apply(lambda x: x+5))\n",
    "        X_dtype_test[f\"log_{col}\"] = np.log(X_dtype_test[col].astype(float).apply(lambda x: x+5))                                         \n",
    "        skewed_cols.append(col)\n",
    "\n",
    "X_dtype_train.drop(columns=skewed_cols, inplace=True)\n",
    "X_dtype_test.drop(columns=skewed_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = scaler.fit_transform(X_dtype_train)\n",
    "X_test_norm = scaler.transform(X_dtype_test)\n",
    "\n",
    "X_train_encoded[num_cols] = X_train_norm\n",
    "X_test_encoded[num_cols] = X_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_norm, X_test_norm, X_dtype_train, X_dtype_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dummy Regression - Baseline\n",
    "\n",
    "Setting baseline metrics to see improvement among other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train_encoded, y_train)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 162.50162595558666\n",
      "Mean Squared Error (MSE): 84570.05314900333\n",
      "Root Mean Squared Error (RMSE): 290.8093071911615\n",
      "R-SQUARED:  0.0\n"
     ]
    }
   ],
   "source": [
    "## print(\"DUMMY REGRESSOR - MEAN\")\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_predict_dummy_mean))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_predict_dummy_mean))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_predict_dummy_mean)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, y_predict_dummy_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dummy_median = DummyRegressor(strategy = 'median').fit(X_train_encoded, y_train)\n",
    "y_predict_dummy_median = lm_dummy_median.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY REGRESSOR - MEDIAN\n",
      "Mean Absolute Error (MAE): 106.54793720058697\n",
      "Mean Squared Error (MSE): 91049.30566297466\n",
      "Root Mean Squared Error (RMSE): 301.7437748537236\n",
      "R-SQUARED:  -0.07661402911212067\n"
     ]
    }
   ],
   "source": [
    "print(\"DUMMY REGRESSOR - MEDIAN\")\n",
    "\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_predict_dummy_median))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_predict_dummy_median))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_predict_dummy_median)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, y_predict_dummy_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression Grid Search on Alpha\n",
    "\n",
    "Testing ridge regression with differing levels of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "linRidge = RidgeCV(alphas=[1e-1, 1, 10], scoring='neg_mean_absolute_error').fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predLinRidge = linRidge.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIDGE REGRESSION GRID SEARCH: \n",
      "Mean Absolute Error (MAE): 141.45532317265173\n",
      "Mean Squared Error (MSE): 69920.79966675368\n",
      "Root Mean Squared Error (RMSE): 264.4254141847067\n"
     ]
    }
   ],
   "source": [
    "print(\"RIDGE REGRESSION GRID SEARCH: \")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, predLinRidge))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, predLinRidge))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, predLinRidge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del linRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost Regressor\n",
    "\n",
    "```n_estimators = 100```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_r = xg.XGBRegressor(objective ='reg:squarederror',\n",
    "                  n_estimators = 100, seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=123,\n",
       "             reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "xgb_r.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model\n",
    "pred = xgb_r.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost n_estimators = 100\n",
      "Mean Absolute Error (MAE): 81.52782540698011\n",
      "Mean Squared Error (MSE): 37576.03206247993\n",
      "Root Mean Squared Error (RMSE): 193.84538184460297\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost n_estimators = 100\")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bayesian Ridge Regressor\n",
    "\n",
    "Testing bayesian ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BayReg = BayesianRidge()\n",
    "BayReg.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = BayReg.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Ridge Regression: \n",
      "Mean Absolute Error (MAE): 141.46817354167044\n",
      "Mean Squared Error (MSE): 69926.25973627747\n",
      "Root Mean Squared Error (RMSE): 264.43573838699916\n"
     ]
    }
   ],
   "source": [
    "print(\"Bayesian Ridge Regression: \")\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression\n",
    "\n",
    "Doing grid search manually because GCP throws TerminatedWorkerError and local throws memory error with GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING 10, 10\n",
      "STARTING 10, 50\n",
      "STARTING 10, 100\n",
      "STARTING 50, 10\n",
      "STARTING 50, 50\n"
     ]
    }
   ],
   "source": [
    "important_features = {}\n",
    "results = {}\n",
    "feature_dict = defaultdict(list)\n",
    "feature_importance = defaultdict(dict)\n",
    "for n_est in (10, 50, 100):\n",
    "    for max_depth in [10, 50, 100]:\n",
    "        print(f\"STARTING {n_est}, {max_depth}\")\n",
    "        rfc = RandomForestRegressor(n_estimators=n_est, max_depth=max_depth, random_state=0, n_jobs=-1)\n",
    "        rfc.fit(X_train_encoded, y_train)\n",
    "        \n",
    "        pred = rfc.predict(X_test_encoded)\n",
    "        \n",
    "        mae = metrics.mean_absolute_error(y_test, pred)\n",
    "        mse = metrics.mean_squared_error(y_test, pred)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "        r2 = metrics.r2_score(y_test, pred)\n",
    "        \n",
    "        results[f\"{n_est}_{max_depth}\"] = {\"mae\": mae, \"mse\": mse, \"rmse\":rmse, \"r2\":r2}\n",
    "        \n",
    "        importances = rfc.feature_importances_\n",
    "        important_feat = np.argsort(importances)[-50:]\n",
    "        \n",
    "        selected_feat_names= list(X_train.columns[important_feat])\n",
    "        \n",
    "        for idx, x in enumerate(important_feat):\n",
    "            feature_dict[selected_feat_names[idx]].append(importances[x])\n",
    "            \n",
    "            try:\n",
    "                feature_importance[selected_feat_names[idx]][\"count\"] += 1\n",
    "                feature_importance[selected_feat_names[idx]][\"importances\"].append(importances[x])\n",
    "            except KeyError:\n",
    "                feature_importance[selected_feat_names[idx]][\"count\"] = 1\n",
    "                feature_importance[selected_feat_names[idx]][\"importances\"] = [importances[x]]\n",
    "            \n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RF Grid Search Results\n",
    "\n",
    "It appears that the sweet spot for metrics optimization vs. model complexity is 10 estimators and max of 50 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gridSearchResults = pd.DataFrame.from_dict(results).T.reset_index()\n",
    "\n",
    "gridSearchResults[['n_estimators', 'max_depth']] = gridSearchResults['index'].str.split(\"_\", expand=True).astype(int)\n",
    "gridSearchMetrics = gridSearchResults.melt(id_vars=[\"index\", \"n_estimators\", \"max_depth\"])\n",
    "\n",
    "metricsChart = alt.Chart(gridSearchMetrics).mark_bar().encode(\n",
    "    x='max_depth:N',\n",
    "    y='value:Q',\n",
    "    color=alt.Color('variable:N', scale = alt.Scale(range=[\"#EAC2B1\", \"#90C6FA\", \"#F8E467\", \"#2C7AAF\"])),\n",
    "    row='n_estimators:N',\n",
    "    column='variable:N',\n",
    ").resolve_scale(\n",
    "  y='independent'\n",
    ").properties(\n",
    "    title={\n",
    "    \"text\": [\"Performance Metrics by n_estimators & max_depth\"],\n",
    "      \"subtitle\": [\"Random Forest Model\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "metricsChart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 Features sorted on number of Grid Search random forest models that consider it important and mean importance among them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x in feature_importance:\n",
    "    feature_importance[x][\"mean_importance\"] = np.mean(feature_importance[x][\"importances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EPOPENYEST',\n",
       " 'Age_interest_adults',\n",
       " 'Age_interest_teens',\n",
       " 'Height_req_inches',\n",
       " 'WDWSEASON_none',\n",
       " 'HSHOURSYEST',\n",
       " 'EPHOURSYEST',\n",
       " 'MKOPENTOM',\n",
       " 'MKFIREN_fantasy in the sky fireworks',\n",
       " 'AKOPEN',\n",
       " 'EPEMHOPEN',\n",
       " 'WDWSEASON_columbus day',\n",
       " 'MKCLOSEYEST',\n",
       " 'MKHOURSYEST',\n",
       " 'MKCLOSETOM',\n",
       " 'WDWSEASON_memorial day',\n",
       " 'WDWSEASON_september low',\n",
       " 'DAYOFYEAR',\n",
       " 'MKOPENYEST',\n",
       " 'HOLIDAYN_lab',\n",
       " 'INSESSION_PLANES',\n",
       " 'MKFIREN_happily ever after',\n",
       " 'Age_of_ride_days',\n",
       " 'MKeventN_mvmcp',\n",
       " 'HSOPENTOM',\n",
       " 'Age_of_ride_years',\n",
       " 'MKeventN_mnsshp',\n",
       " 'WDWSEASON_thanksgiving',\n",
       " 'inSession_NY_NJ',\n",
       " 'WDWeventN_wdwdd|wdwhol',\n",
       " 'EPCLOSEYEST',\n",
       " 'Wind Speed Quality_p',\n",
       " 'Ride_type_slow',\n",
       " 'MKPRDNN_main street electrical parade',\n",
       " 'MKEMHCLOSE',\n",
       " 'Ride_name_dumbo the flying elephant',\n",
       " 'HSEMHEYEST',\n",
       " 'inSession',\n",
       " 'MKEMHMORN',\n",
       " 'MKPRDNN_mickey\\'s \"boo-to-you\" halloween parade',\n",
       " 'MKPRDNN_none',\n",
       " \"MKFIREN_disney's celebrate america! - a fourth of july concert in the sky\",\n",
       " 'HSHOURSTOM',\n",
       " 'WEEKOFYEAR',\n",
       " \"MKFIREN_disney's not so spooky spectacular\",\n",
       " 'DAYOFWEEK',\n",
       " 'inSession_NY_NJ_PA',\n",
       " 'MONTHOFYEAR',\n",
       " 'Age_interest_tweens',\n",
       " 'inSession_New_England']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(feature_importance, key=lambda x: (-feature_importance[x]['count'], -feature_importance[x][\"mean_importance\"]))[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters seem to have the best balance of simplicity and low error.\n",
    "\n",
    "##### ```n_estimators=10``` ```max_depth=50```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create regressor object\n",
    "# rf = RandomForestRegressor(n_estimators = 10, max_depth=50, n_jobs=-1, random_state = 0)\n",
    " \n",
    "# # fit the regressor with x and y data\n",
    "# rf.fit(X_train_encoded, y_train)\n",
    "\n",
    "# joblib.dump(rf, 'random_forest10_50_pkl' + '.gz', compress=('gzip', 4))  \n",
    "\n",
    "#########\n",
    "# OR \n",
    "#########\n",
    "\n",
    "# load saved model\n",
    "rf = joblib.load('random_forest10_50_pkl'  + '.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST n_estimators = 100 : \n",
      "Mean Absolute Error (MAE): 32.60095335124882\n",
      "Mean Squared Error (MSE): 14949.679666757667\n",
      "Root Mean Squared Error (RMSE): 122.26888265931633\n",
      "R-SQUARED:  0.823227264142569\n"
     ]
    }
   ],
   "source": [
    "print(\"RANDOM FOREST n_estimators = 100 : \") \n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "print(\"R-SQUARED: \", metrics.r2_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "#\n",
    "# Sort the feature importance in descending order\n",
    "#\n",
    "sorted_indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CapacityLostWGT_EP', 'Ride_duration_min', 'Age_of_ride_years',\n",
       "       'Age_of_ride_days', 'Ride_name_walt disney's carousel of progress',\n",
       "       'AKHOURSTOM', 'CapacityLostWGT_AK', 'CapacityLost_AK',\n",
       "       'EPOPENYEST_HOUR', 'EPOPEN_HOUR', 'Cloud Height', 'EPCLOSE_HOUR',\n",
       "       'WDW_TICKET_SEASON_none', 'Ride_name_prince charming regal carrousel',\n",
       "       'EPEMHOPEN_HOUR', 'AKHOURS', 'YEAR', 'Weather Type', 'HOLIDAYPX',\n",
       "       'HSHOURSTOM', 'Park_area_fantasyland',\n",
       "       'Ride_name_tomorrowland transit authority peoplemover', 'DAYOFWEEK',\n",
       "       'Visibility Distance (M)', 'MKEMHMORN', 'TA_Stars', 'MKeventN_dah',\n",
       "       'EPCLOSEYEST_HOUR', 'HSHOURSYEST', 'MKFIREN_happily ever after',\n",
       "       'MKFIREN_happy hallowishes fireworks', 'EPEMHCLOSE_HOUR',\n",
       "       'inSession_New_Jersey', 'WEATHER_WDWHIGH', 'Ride_name_jungle cruise',\n",
       "       'inSession_dlr', 'CapacityLostWGT_HS', 'inSession_Nothwest',\n",
       "       'Park_area_tomorrowland', 'AKHOURSYEST', 'Height_req_inches',\n",
       "       'HOLIDAYM', 'MKCLOSETOM_HOUR', 'INSESSION_PLANES', 'Ride_type_dark',\n",
       "       'HSHOURSEMHYEST', 'EPHOURSEMH', 'HSFIRET2_HOUR', 'TL_rank',\n",
       "       'HSHOURSEMH'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded.columns[sorted_indices[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
